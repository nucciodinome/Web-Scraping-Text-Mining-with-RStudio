# Scraping the Web with R

**Welcome to this short tutorial on Web Scraping!**

## A Case Study: The Wind Energy Across Europe 
We want to explore the relational network that the European organizational actors involveed the wind energy sector eventually build within the digital environment.
In order to do so, we have identified a promising source of information, the Wind Energy Europe, wich collect more than 600 organization involved in wind energy project.

Our formal research question is:
***RQ: Which organizations build the densest relational network in the digital environment?***

Let's try to address it through R-based tools.
You can use the both your local R Studio or an [R Studio Cloud Project](https://posit.cloud/) to start your scraping task!

## The sampling procedure


Now, you can open R Studio and import the file.
As you can see, the dataset is a list of URLs, named SEED, which refer to the websites of 50 members of Wind Energy Europe.

Given RQ, you should first identify what data you need to track relationships between organizations.
As we know, information about the relationships between two websites is encoded as hyperlinks, which, in the HTML language are reported with the "a" tag, while the target URL of the link is made explicit with the "href" attribute.
We can assume, moreover, that not all website links are on the homepage: many of them will be found on the internal pages of the website (e.g., "about" page, "partners" page, "clients" page, etc.).

Let's assume, then, that we also want to investigate the internal pages which are directly linked to the homepage, or, in other words, at 1 degree of separation from the homepage (let's call them second-level web pages).

We can then start to build our procedure aimed at extracting the data we need and then identify an R script for web scraping.

Probably, your procedure will look like this:

- [x] Starting from the list of 50 SEEDs...
- [ ] __STEP I__: Extract all the _Second Level web pages_ in order to...
- [ ] __STEP II__: Collect all _external links_ (which refer to other organizations' websites connected to the SEEDs


### STEP Ia: Extract Second Level web pages with Rvest
Having the SEEDs it should be easy to extract all the internal links which refer to second level web pages.
*N.b. The :x: are the fields you need to fill in.*



#### Import R packages
Let's try the R package "Rvest." (and some other additional packages)

```markdown

install.packages(rvest)
install.packages( (stringr)
install.packages( (dplyr)

#then activate them:

library(rvest)
library(stringr)
library(dplyr)

```

